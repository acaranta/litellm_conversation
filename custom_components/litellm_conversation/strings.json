{
  "config": {
    "step": {
      "user": {
        "title": "LiteLLM Conversation",
        "description": "Configure your LiteLLM endpoint and credentials",
        "data": {
          "base_url": "Base URL",
          "api_key": "API Key",
          "chat_model": "Model",
          "prompt": "System Prompt"
        },
        "data_description": {
          "base_url": "The base URL for your LiteLLM proxy endpoint (e.g., https://api.openai.com/v1)",
          "api_key": "The API key for authentication with your LiteLLM proxy",
          "chat_model": "The model to use for conversations",
          "prompt": "Custom prompt to use for the conversation agent"
        }
      }
    },
    "error": {
      "cannot_connect": "Failed to connect to the LiteLLM endpoint",
      "invalid_auth": "Invalid authentication credentials",
      "unknown": "Unexpected error occurred"
    },
    "abort": {
      "already_configured": "Device is already configured"
    }
  },
  "options": {
    "step": {
      "init": {
        "title": "LiteLLM Conversation Options",
        "description": "Configure advanced options for your LiteLLM integration",
        "data": {
          "prompt": "System Prompt",
          "chat_model": "Model",
          "vision_model": "Vision Model",
          "max_tokens": "Max Tokens",
          "top_p": "Top P",
          "temperature": "Temperature", 
          "presence_penalty": "Presence Penalty",
          "frequency_penalty": "Frequency Penalty"
        },
        "data_description": {
          "prompt": "The system prompt template to use for conversations. You can use Home Assistant template syntax.",
          "chat_model": "The model to use for chat completions",
          "vision_model": "The vision model to use for AI Task image analysis and file processing",
          "max_tokens": "Maximum number of tokens to generate in the completion",
          "top_p": "An alternative to sampling with temperature, called nucleus sampling",
          "temperature": "Controls randomness: Lowering results in less random completions",
          "presence_penalty": "Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far",
          "frequency_penalty": "Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far"
        }
      }
    }
  }
}